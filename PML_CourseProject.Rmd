---
title: "Identification of Correct Barbell Exercise Execution Using Machine Learning"
author: "Sam Vacik"
date: "12/17/2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Human Activity Recognition (HAR) aims to identify movement type and performance based on physical data gathered by personal electronic devices such as Jawbone Up, Nike Fuelband, or Google Fitbit. Many users wear these devices during exercise, gain data with the desire to identify correct or poor execution, and then improve their technique In this data analysis, ten machine learning models will be built and tested to determine the best model capable of identifying the correct execution of a barbell lift independent of individual users. The best model will be used in the assessment to gauge effectiveness in apply practical machine learning to a real case to complete Practical Machine Learning through Coursera.

The dataset was available for download through the Practical Machine Learning Course Project web page on Coursera and information about the data was provided through the Human Activity Recognition data website (see the link in the following section). It was split into a different training and testing sets for this project. Only the training data is used in this project.

## Setup, Exploration & Data Cleaning

The [Human Activity Recognition dataset](https://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) is utilized in this data analysis (see citation). The HAR dataset was collected from six  participants who performed 10 repetitions of the dumbbell biceps curl in five ways, as documented in the "classe" variable in the dataset. Each participant worse a series of three-axial sensors positioned on the upper arm, wrist, waist, and one end of the barbell. Each variant of the biceps curl was assigned a classe letter from A to E. The class A variant represents correct execution of a biceps curl and the others correlate to incorrect execution. Each study participant possessed minimal experience of the dumbbell biceps curl and received supervision of their exercise execution by professional fitness experts. This study utilizes the training data set (file name "pml-training.csv") available through the HAR dataset.

This study utilizes the caret and ggplot2 packages in addition to custom functions written for conversion of alphabet letters to numbers, rounding numbers in a vector, and creation of a table containing the performance metrics against the test data.

Data exploration began by reading in the input training data set using the function "read.csv". The first seven columns contain data related to the user, date, time, and windows conducted. As the goal of this exercise is to build a prediction model of correct movements independent of user and temporal data, these columns will be removed from the dataset. The pml data set is `r dim(read.csv("pml-training.csv"))` (rows by columns). Within the dataset, there are `r sum(is.na(read.csv("pml-training.csv")))` NAs and `r sum(grep("#DIV/0!", read.csv("pml-training.csv")))` indivisible numbers (i.e. divide by 0). Data cleaning removed the rows and columns associated with the NAs and indivisible numbers, reducing the dimensions of the dataset to 406 120 (rows by columns). In other words, the number of predictors reduced from 159 to 119 (a difference of 40). Lastly, the classe variable contains alphabetic letters (from "A" to "E" corresponding with various movements) that proved difficult to handle for the machine learning algorithms; the letters were converted to numbers of 1 to 5 (i.e. A replaced by 1, and so forth).

The reduced dataset contains 119 predictors and 406 rows of entries post data cleaning and remains excessive for potential feature selection, which could introduce bias into the results. This data analysis utilized a leave-one out (cross-validation) approach using random sampling with a chosen error rate of 0.8 to split the data set into a training and test set (using the function createDataPartition in the caret package). The resulting dimensions of the training set and the test set were 327 by 120 and 79 by 120 (rows by columns), respectively. 

```{r, include=FALSE}
# Function definitions
CapLet2num <- function(x) {utf8ToInt(x) - utf8ToInt("A") + 1L}
Round2Int <- function(x) {a <- round(x, digits=0);for(i in 1:length(a[])){ if(a[[i]] <=0){a[[i]] <- 1}; if(a[[i]] > 5){a[[i]] <- 5} }; return(a)}
modagg <- function(x,y) { testright <- y == x$classe; return(table(x, testright))  }
modacc <- function(x,y){return(table(x,y))}
PerfMets <- function(preds, testdat) {return(data.frame( R2 = R2(preds, testdat$classe),RMSE = RMSE(preds, testdat$classe), MAE = MAE(preds,testdat$classe)))}
ErrStat <- function(predz, testz) {
     ErrorMatrix <- data.frame(matrix(data = 0, nrow=1, ncol=9))
     names(ErrorMatrix) <- c("TP", "FP", "FN", "TN", "Sens", "Specs", "PosPredVal", "NegPredVal", "Acc")
     for(i in 1:length(predz[])){
          if(predz[[i]] == 1 & testz[i,120] == 1){
               # True Positive
               ErrorMatrix[1,1] <- ErrorMatrix[1,1] + 1
          } 
          else if(predz[[i]] == 1 & testz[i,120] > 1){
               # False Positive
               ErrorMatrix[1,2] <- ErrorMatrix[1,2] + 1
          } 
          else if(predz[[i]] > 1 & testz[i,120] > 1){  #& predz[[i]] == testz[i,120]){
               # True Negative
               ErrorMatrix[1,4] <- ErrorMatrix[1,4] + 1
          } 
          else if(predz[[i]] > 1 & testz[i,120] == 1){
               # False Negative
               ErrorMatrix[1,3] <- ErrorMatrix[1,3] + 1
          }
     }
     #Calculate Sensitivity ("Sens")
     ErrorMatrix[1,5] <- ErrorMatrix[1,1] / (ErrorMatrix[1,1] + ErrorMatrix[1,3])
     #Calculate Specificity ("Specs")
     ErrorMatrix[1,6] <- ErrorMatrix[1,4] / (ErrorMatrix[1,2] + ErrorMatrix[1,4])
     #Calculate Positive PRedictive Value
     ErrorMatrix[1,7] <- ErrorMatrix[1,1] / (ErrorMatrix[1,1] + ErrorMatrix[1,2])
     # Calculate Negative Predictive Value
     ErrorMatrix[1,8] <- ErrorMatrix[1,4] / (ErrorMatrix[1,3] + ErrorMatrix[1,4])
     # Calculate Accuracy
     ErrorMatrix[1,9] <- (ErrorMatrix[1,1] + ErrorMatrix[1,4]) / (ErrorMatrix[1,1] + ErrorMatrix[1,2] + ErrorMatrix[1,3] + ErrorMatrix[1,4])
     return(ErrorMatrix)
}


# Import libraries
library(caret); library(ggplot2)

#Load input data file using read library csv function.
trains <- read.csv("pml-training.csv")
#tests <- read.csv("pml-testing.csv")
dim(trains)

# Remove teh columns identifying user names, date, time, etc. in the first seven rows.
trains <- trains[,8:length(trains[1,])]

# Clean data and subset data to remove NAs and other non-numbers.
#Find any NAs.
sum(is.na(trains))
CCs <- complete.cases(trains); trains <- trains[CCs == 1,]
trains <- trains[-grep("#DIV/0!",trains)]

# Using classe as letters.
#trains$classe <- factor(as.character(trains$classe))

# Using classe as numbers.
# Convert classe variable type character to numbers. Replace A, B, C... with 1, 2, 3...
trains$classe <- unname(sapply(trains$classe, CapLet2num)) 

```
```{r echo=FALSE, include=FALSE}
# 1. Set error rate
lilp <- 0.8
seed <- 1

# 2. Split data into >> Training, Testing, & Validation (optional)
set.seed(seed)
inTrain <- createDataPartition(y=trains$classe, p = lilp, list=FALSE)
trainset <- trains[inTrain,]; testset <- trains[-inTrain,]
```

Initial data exploration occurred with a few variables such as total acceleration and roll per sensor. As can be seen in Figure 1, the data are fairly scattered in relation to classe, but there still appear possible linear and polynomial trends between some predictors and classe. The random sampling of the training data set into new training and test sets naturally resulted in different distributions as seen in Figure 2.

```{r echo=FALSE, cache = TRUE, fig.width=7, fig.height=4}
par(mfrow=c(1,2))
featurePlot(x=trainset[,c("total_accel_belt", "total_accel_arm", "total_accel_forearm", "total_accel_dumbbell")], y=trainset$classe, plot="pairs")
featurePlot(x=trainset[,c("roll_belt", "roll_arm", "roll_forearm", "roll_dumbbell")], y=trainset$classe, plot="pairs")
```
Figure 1 Comparison of Total Acceleration and Roll by Sensor against Classe (Y): (Left) Total Acceleration by Sensor VS Classe: Scatter plots showing various trends of total acceleration against the classe variable (represented in the chart as y) per sensor (belt, arm, forearm, and dumbbell). (Right) Roll Movement by Sensor VS Classe: Scatter plots showing various trends of roll  against the classe variable (represented in the chart as y) per sensor (belt, arm, forearm, and dumbbell).

```{r echo=FALSE, fig.width=7, fig.height=4}
par(mfrow=c(1,2))
hist(trainset$classe)
hist(testset$classe)
```
Figure 2 Histograms of the training and test sets as split from the original training set.

## Data Analysis 

The data analysis used a seed of 2134 to ensure reproducibility of the results. Each of the models were established using the "train" function from the R caret package. 

This experiment tested ten machine learning methods available through the train function of the R package caret to determine which model will perform best in identifying correct barbell exercise execution (independent of user). The data analysis  ran the train function with the corresponding model (parameter "method" in the function call) and then predicted movement type of the test data. 

In this experiment, ten machine learning methods available through the R package caret are tested to determine which may perform best in identifying correct barbell exercise execution. The methods applied include: the general linear model (GLM); k-nearest neighbors (KKNN); bayesian ridge regression (BRIDGE); random forest (RF); treebagging (TBAG); model tree (M5); quantile random forest (QRF); multivariate adaptive regression spline (earth); cubist (cubist); and bayesian random neural networks (BRNN). 

The best performing algorithm will be chosen based on which achieves the best performance and error metrics against the training and testing sets. The performance metrics include the r-squared (R^2), root mean square error (RMSE), and the mean absolute error (MAE). The error metrics include the counts of true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) in addition to sensitivity (Sens), specificity (Specs), positive predictive value (PosPredVal), negative predictive value (NegPredVal), and the accuracy (acc). The best algorithm will have the highest R^2, lowest RMSE, and lowest MAE while maximizing the TP/TN values and minimizing the FP/FN values. 
```{r echo=FALSE, include=FALSE}
# 3. On the training set, pick features. (Use cross-validation)
# General Linear REgression / Model
set.seed(seed)
fitglm <- train(classe ~., data=trainset, method="glm")
preglm <- predict(fitglm, newdata=testset)
preglm2 <- Round2Int(preglm)
glmperf <- PerfMets(preglm2, testset)
glmresults <- fitglm$results
glmerr <- ErrStat(preglm2, testset)
```
```{r echo=FALSE, include=FALSE}
# Linear Model / Linear REgression
#method = qrnn "Quantized regression neural network"; tp 13; acc 0.53
# method = RRF; tp = 10; acc = 0.58
# kknn TP FP FN TN      Sens     Specs PosPredVal NegPredVal
#1 13  2 30 34 0.3023256 0.9444444  0.8666667    0.53125
#        Acc
#1 0.5949367
set.seed(seed)
fitlm <- train(classe ~ ., data=trainset, method="kknn")
predlm <- predict(fitlm, newdata=testset)
predlm2 <- Round2Int(predlm)
lmperf <- PerfMets(predlm2, testset)
lmresults <- fitlm$results
lmerr <- ErrStat(predlm2, testset)
```
```{r echo=FALSE, include=FALSE}
# CART
set.seed(seed)
fitrpart <- train(classe ~., data=trainset, method="bridge")
predrpart <- predict(fitrpart, newdata=testset)
predrpart2 <- Round2Int(predrpart)
rpartperf <- PerfMets(predrpart2, testset)
rpartresults <- fitrpart$results
rparterr <- ErrStat(predrpart2, testset)
```
```{r echo=FALSE, include=FALSE}
# Random Forests
set.seed(seed)
fitrf <- train(classe ~ ., data=trainset, method="rf", prox=TRUE)
predrf <- predict(fitrf, testset)
predrf2 <- Round2Int(predrf)
rfperf <- PerfMets(predrf2, testset)
rfresults <- fitrf$results
rferr <- ErrStat(predrf2, testset)
```
```{r echo=FALSE, include=FALSE}
set.seed(seed)
fittbag <- train(classe ~ ., data=trainset, method="treebag")
predtbag <- predict(fittbag, testset)
predtbag2 <- Round2Int(predtbag)
tbagperf <- PerfMets(predtbag2, testset)
tbagresults <- fittbag$results
tbagerr <- ErrStat(predtbag2, testset)
```
```{r echo=FALSE, include=FALSE}
set.seed(seed)
# Model Tree
fitqreg <- train(classe ~ ., data=trainset, method="M5")
predqreg <- predict(fitqreg, testset)
predqreg2 <- Round2Int(predqreg)
qregperf <- PerfMets(predqreg2, testset)
qregresults <- fitqreg$results
qregerr <- ErrStat(predqreg2, testset)
```
```{r echo=FALSE, include=FALSE}
set.seed(seed)
# Quantile RF
fitqrf <- train(classe ~ ., data=trainset, method="qrf")
predqrf <- predict(fitqrf, testset)
predqrf2 <- Round2Int(predqrf)
qrfperf <- PerfMets(predqrf2, testset)
qrtresults <- fitqrf$results
qrferr <- ErrStat(predqrf2, testset)
```
```{r echo=FALSE, include=FALSE}
set.seed(seed)
# Multivariate Adaptive Regression Splines
fitearth <- train(classe ~ ., data=trainset, method="earth")
predearth <- predict(fitearth, testset)
predearth2 <- Round2Int(predearth)
earthperf <- PerfMets(predearth2, testset)
earthresults <- fitearth$results
eartherr <- ErrStat(predearth2, testset)
```
```{r echo=FALSE, include=FALSE}
set.seed(seed)
# Self-Organizing Maps
fitxyf <- train(classe ~ ., data=trainset, method="cubist")
predxyf <- predict(fitxyf, testset)
predxyf2 <- Round2Int(predxyf)
xyfperf <- PerfMets(predxyf2, testset)
xyfresults <- fitxyf$results
xyferr <- ErrStat(predxyf2, testset)
```
```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
set.seed(seed)
# Bayesian Regularlized Neural Networks
fitbrnn <- train(classe ~ ., data=trainset, method="brnn")
predbrnn <- predict(fitbrnn, testset)
predbrnn2 <- Round2Int(predbrnn)
brnnperf <- PerfMets(predbrnn2, testset)
brnnresults <- fitbrnn$results
brnnerr <- ErrStat(predbrnn2, testset)
```
```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
set.seed(seed)
# 4. On the training set, pick prediction function. (Use cross-validation.)

#. 5. If no validation, apply 1x to test set

# 6. If validation, apply to test set and refine. Apply 1x to validation.

# Build a data frame of the performance metrics.
PerfMetrics <- data.frame(matrix(nrow=10, ncol=7))
names(PerfMetrics) <- c("Method", "Train-R2", "Train-RMSE", "Train-MAE", "Test-R2", "Test-RMSE", "Test-MAE")
PerfMetrics[1,] <- c("GLM", glmresults$Rsquared[1], glmresults$RMSE[1], glmresults$MAE[1], glmperf)
PerfMetrics[2,] <- c("KKNN", lmresults$Rsquared[1], lmresults$RMSE[1], lmresults$MAE[1], lmperf)
PerfMetrics[3,] <- c("Bridge", rpartresults$Rsquared[1], rpartresults$RMSE[1], rpartresults$MAE[1], rpartperf)
PerfMetrics[4,] <- c("RF", rfresults$Rsquared[1], rfresults$RMSE[1], rfresults$MAE[1],rfperf)
PerfMetrics[5,] <- c("TBAG", tbagresults$Rsquared[1], tbagresults$RMSE[1], tbagresults$MAE[1],tbagperf)
PerfMetrics[6,] <- c("RQNC", qregresults$Rsquared[1], qregresults$RMSE[1], qregresults$MAE[1], qregperf)
PerfMetrics[7,] <- c("QRF", qrtresults$Rsquared[1], qrtresults$RMSE[1], qrtresults$MAE[1],qrfperf)
PerfMetrics[8,] <- c("MARS-Earth", earthresults$Rsquared[1], earthresults$RMSE[1], earthresults$MAE[1], earthperf)
PerfMetrics[9,] <- c("Cubist", xyfresults$Rsquared[1], xyfresults$RMSE[1], xyfresults$MAE[1], xyfperf)
PerfMetrics[10,] <- c("BRNN", brnnresults$Rsquared[1], brnnresults$RMSE[1], brnnresults$MAE[1],brnnperf)

glmerr <- ErrStat(preglm2, testset)
lmerr <- ErrStat(predlm2, testset)
rparterr <- ErrStat(predrpart2, testset)
rferr <- ErrStat(predrf2, testset)
tbagerr <- ErrStat(predtbag2, testset)
qregerr <- ErrStat(predqreg2, testset)
qrferr <- ErrStat(predqrf2, testset)
eartherr <- ErrStat(predearth2, testset)
xyferr <- ErrStat(predxyf2, testset)
brnnerr <- ErrStat(predbrnn2, testset)

Errors <- data.frame(matrix(0, nrow=10, ncol=10))
names(Errors) <- c("Method", "TP", "FP", "FN", "TN", "Sens", "Specs", "PosPredVal", "NegPredVal", "Acc")

Errors[1,] <- c("GLM", glmerr)
Errors[2,] <- c("KKNN", lmerr)
Errors[3,] <- c("Brridge", rparterr)
Errors[4,] <- c("RF", rferr)
Errors[5,] <- c("TBAG", tbagerr)
Errors[6,] <- c("RQNC", qregerr)
Errors[7,] <- c("QRF", qrferr)
Errors[8,] <- c("MARS-Earth", eartherr)
Errors[9,] <- c("Cubist", xyferr)
Errors[10,] <- c("BRNN", brnnerr)

```

The performance and error metrics are captured in Tables 1 and 2, respectively. As seen in Table 1, the performance ranges by method and by the randomly sampled leave-one-out method, but the performance varies across the data split. Overall, the top three performers across the training data set in descending order of r-squared value are the quantile random forest (QRF), random forest (RF), and treebagging (TBAG). The worst three performers in order of increasing r-squared value for the training set are the general linear model (GLM), the quantile regression neural network (QRNN), and the quantile regression (QRNC). These results vary when the models were used to predict values based on the test data. The best perdictors on the test data in order of descending r-squared value are quantile random forest (QRF), random forest (RF), and cubist (cubist) and the worst performers in order of increasing r-squared are quantile regression neural network (QRNN), quantile regression (RQNC), and the multivariate adaptive regression (MARS-earth). 

```{r PerfMetrics, echo=FALSE}
print(PerfMetrics)
```
Table 1 R2, RMSE, & MAE by Method: Comparison table showing the R2, RMSE, and MAE values by method against the training and testing datasets.

The trends found in the performance metrics reflect in the error metrics. The top performers according to the error metrics include random forest (RF), quantile random forest (QRF), and cubist, which produced the fewest FP and FN values of the ten algorithms tested.  The differences between the random forest and quantile random forest models is very small (a difference of `r Errors[4,10] - Errors[7,10]` in terms of accuracy). When comparing the ML algorithms by their performance and error metrics, it becomes apparent that the best performing algoirhtms are quantile random forest (QRF), random forest (RF), and cubist.
```{r Errors, echo=FALSE}
print(Errors)
```
Table 2 Error Statistics by Method: Comparison showing values of true positives (TP), false positives (FP), false negatives (FN), true negatives (TN), sensitivity (Sens), specificity (Specs), positive predictive value (PosPredVal), negative predictive value (NegPredVal), and accuracy (Acc) by ML algorithm.

The final models of the random forest and quantile random forest models are shown in Figure 4. The two models possess very similar performance and error metrics. The quantile random forest model performs better in terms of its performance metrics with a higher r-squared value and lower RMSE/MAE values than the random forest model and the random forest model finds one additional true positive compared to the quantile random forest. When examining their error versus number of trees plots, the differences are again slight, but the RF model achieves a smaller error rate than the QRF model.
```{r fitrf, echo=FALSE, error=TRUE, fig.width=8, fig.height=4}
par(mfrow=c(1,2))
plot(fitrf$finalModel)
plot(fitqrf$finalModel)
```
Figure 4 RF Error Plot: Graph depicting error rate decreasing with increased number of trees when implementing the quantile random forest machine learning algorithm.

## Conclusions

Ten different ML algorithms were tested and compared to find the best predictor of activity recognition of a correct barbell curl. When conducting the data analysis, the quantile random forest (QRF) and random forest (RF) models produced the greatest number of true positives/false negatives and the lowest number of false positives/false negatives while achieving the highest r-squared values of the other methods tested. Between these two models, there are slight differences in their error statistics but greater difference in their performance metrics. The differences in r-squared, RMSE, and MAE values between these two algorithms are significant and the quantile random forest (QRF) model performed better than the random forest model by these metrics. The random forest (RF) model achieved one greater true positive than the quantile random forest (QRF) model and achieves a smaller error rate as the number of trees increases. In conclusion, the  random forest (RF) model performs best and will be used to test against the quiz data set (aka the test data set included in the file "pml-testing.csv") for the assessment in this final project of the Practical Machine Learning course on Coursera.

## Citation

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 